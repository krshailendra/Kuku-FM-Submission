{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11310353,"sourceType":"datasetVersion","datasetId":7073825}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install openai-whisper","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T12:30:46.223677Z","iopub.execute_input":"2025-04-07T12:30:46.224022Z","iopub.status.idle":"2025-04-07T12:30:49.639830Z","shell.execute_reply.started":"2025-04-07T12:30:46.223995Z","shell.execute_reply":"2025-04-07T12:30:49.638959Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: openai-whisper in /usr/local/lib/python3.10/dist-packages (20240930)\nRequirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.60.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (1.26.4)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.5.1+cu121)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (4.67.1)\nRequirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (10.5.0)\nRequirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.9.0)\nRequirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (3.2.0)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (0.43.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->openai-whisper) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->openai-whisper) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->openai-whisper) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->openai-whisper) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->openai-whisper) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->openai-whisper) (2.4.1)\nRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\nRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2.32.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->openai-whisper) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->openai-whisper) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->openai-whisper) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->openai-whisper) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->openai-whisper) (2024.2.0)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import whisper\n\ndef transcribe_audio(file_path):\n    # Load the model (base is a good starting point; options: tiny, base, small, medium, large)\n    model = whisper.load_model(\"base\")\n\n    # Transcribe the audio file\n    print(\"Transcribing...\")\n    result = model.transcribe(file_path)\n\n    # Print the transcribed text\n    print(\"Transcription Complete:\")\n    print(result[\"text\"])\n\n    return result[\"text\"]\n\n# Example usage\nif __name__ == \"__main__\":\n    audio_file = \"/kaggle/input/dffdgd/WhatsApp Audio 2025-04-07 at 17.46.48_fc3a037c.waptt.opus\"  # Replace with your file path\n    transcribe_audio(audio_file)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T12:31:13.367650Z","iopub.execute_input":"2025-04-07T12:31:13.368012Z","iopub.status.idle":"2025-04-07T12:31:15.218989Z","shell.execute_reply.started":"2025-04-07T12:31:13.367982Z","shell.execute_reply":"2025-04-07T12:31:15.218321Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(fp, map_location=device)\n","output_type":"stream"},{"name":"stdout","text":"Transcribing...\nTranscription Complete:\n Hello Sir, Myself Divyam\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import openai\n\n# NEW client setup for openai v1.x\nclient = openai.OpenAI(api_key=\"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\")\n\ndef convert_transcript_to_script(transcript):\n    prompt = f\"\"\"\nYou are a professional scriptwriter.\nI will give you a raw transcript of a conversation.\nYour task is to structure it into a proper script format with the following rules:\n\n- Identify and label different speakers (use Speaker 1, Speaker 2 if needed).\n- Format the dialogues like a screenplay.\n- Add basic narrative cues where helpful (like (laughs), (sighs), etc.).\n- Maintain natural flow and break into paragraphs for readability.\n- If you can identify speakers from the script then allot the name as well or else give suitable name according to you\n\nTranscript:\n\\\"\\\"\\\"\n{transcript}\n\\\"\\\"\\\"\n\nNow give me the formatted script:\n\"\"\"\n\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        temperature=0.7\n    )\n\n    return response.choices[0].message.content\n\n\n# Example usage\nif __name__ == \"__main__\":\n    transcript = \"\"\"\nhey man where were you last night\noh i was stuck at work. didn’t get out till midnight\ndude we waited for you at the bar\ni know i’m sorry. next round’s on me\ndamn right it is (laughs)\n    \"\"\"\n\n    result = convert_transcript_to_script(transcript)\n    print(result)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T12:36:26.386585Z","iopub.execute_input":"2025-04-07T12:36:26.386893Z","iopub.status.idle":"2025-04-07T12:36:31.168055Z","shell.execute_reply.started":"2025-04-07T12:36:26.386868Z","shell.execute_reply":"2025-04-07T12:36:31.167270Z"}},"outputs":[{"name":"stdout","text":"SPEAKER 1 (JOHN)\nHey man, where were you last night?\n\nSPEAKER 2 (MIKE)\nOh, I was stuck at work. Didn’t get out till midnight.\n\nJOHN\nDude, we waited for you at the bar.\n\nMIKE\nI know, I’m sorry. Next round’s on me.\n\nJOHN\n(laughs) Damn right it is.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}