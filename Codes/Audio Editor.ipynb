{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10948039,"sourceType":"datasetVersion","datasetId":6809756}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install gtts\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T09:05:25.440595Z","iopub.execute_input":"2025-04-08T09:05:25.441013Z","iopub.status.idle":"2025-04-08T09:05:31.323219Z","shell.execute_reply.started":"2025-04-08T09:05:25.440982Z","shell.execute_reply":"2025-04-08T09:05:31.322010Z"}},"outputs":[{"name":"stdout","text":"Collecting gtts\n  Downloading gTTS-2.5.4-py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from gtts) (2.32.3)\nRequirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.10/dist-packages (from gtts) (8.1.7)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (2025.1.31)\nDownloading gTTS-2.5.4-py3-none-any.whl (29 kB)\nInstalling collected packages: gtts\nSuccessfully installed gtts-2.5.4\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport librosa\nfrom transformers import AutoTokenizer, BertForSequenceClassification\nfrom pydub import AudioSegment, silence\nfrom pydub.generators import Sine\nimport nltk\nfrom gtts import gTTS\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-08T09:05:33.794663Z","iopub.execute_input":"2025-04-08T09:05:33.795037Z","iopub.status.idle":"2025-04-08T09:06:01.219911Z","shell.execute_reply.started":"2025-04-08T09:05:33.795005Z","shell.execute_reply":"2025-04-08T09:06:01.218886Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Download NLTK punkt tokenizer for sentence splitting\nnltk.download(\"punkt\")\nfrom nltk.tokenize import sent_tokenize","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T09:06:05.154592Z","iopub.execute_input":"2025-04-08T09:06:05.155276Z","iopub.status.idle":"2025-04-08T09:06:05.316955Z","shell.execute_reply.started":"2025-04-08T09:06:05.155239Z","shell.execute_reply":"2025-04-08T09:06:05.315925Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Load emotion detection model\ntokenizer = AutoTokenizer.from_pretrained(\"bhadresh-savani/bert-base-go-emotion\")\nmodel = BertForSequenceClassification.from_pretrained(\"bhadresh-savani/bert-base-go-emotion\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T09:06:07.230620Z","iopub.execute_input":"2025-04-08T09:06:07.230986Z","iopub.status.idle":"2025-04-08T09:06:29.624656Z","shell.execute_reply.started":"2025-04-08T09:06:07.230957Z","shell.execute_reply":"2025-04-08T09:06:29.622968Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/333 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8a57911f6704c51aade42fb86676001"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0116aec2fc1b4274904e45a97d359f90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7f1697be9da42db9f57fc935bad3a0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c88d3810f6894771917c12f2ddf3058b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e71a05fa02145cb98f6f1ee8ce42338"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"475e66f9cc0c4cf6924fdb0d9a5c4a72"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# Define emotion labels\nemotion_labels = [\n    \"admiration\", \"amusement\", \"anger\", \"annoyance\", \"approval\", \"caring\", \n    \"confusion\", \"curiosity\", \"desire\", \"disappointment\", \"disapproval\", \n    \"disgust\", \"embarrassment\", \"excitement\", \"fear\", \"gratitude\", \"grief\",\n    \"joy\", \"love\", \"nervousness\", \"optimism\", \"pride\", \"realization\", \n    \"relief\", \"remorse\", \"sadness\", \"surprise\", \"neutral\"\n]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T09:06:34.730812Z","iopub.execute_input":"2025-04-08T09:06:34.731211Z","iopub.status.idle":"2025-04-08T09:06:34.736601Z","shell.execute_reply.started":"2025-04-08T09:06:34.731178Z","shell.execute_reply":"2025-04-08T09:06:34.735541Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Function to generate speech from text\n#This is just for testing the model, the input we will have should be an audio only in this case.\ndef generate_speech(text, output_file=\"generated_narration.mp3\"):\n    tts = gTTS(text)\n    tts.save(output_file)\n    print(f\"Generated speech saved as {output_file}\")\n    return output_file","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T09:06:37.260818Z","iopub.execute_input":"2025-04-08T09:06:37.261212Z","iopub.status.idle":"2025-04-08T09:06:37.266168Z","shell.execute_reply.started":"2025-04-08T09:06:37.261180Z","shell.execute_reply":"2025-04-08T09:06:37.265026Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Function to detect emotions for sentences\ndef detect_emotions(text):\n    sentences = sent_tokenize(text)\n    sentence_emotion_map = {}\n\n    for i, sentence in enumerate(sentences):\n        inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True)\n        with torch.no_grad():\n            logits = model(**inputs).logits\n        predicted_class = torch.argmax(logits, dim=1).item()\n        detected_emotion = emotion_labels[predicted_class]\n\n        sentence_emotion_map[i + 1] = (sentence, detected_emotion)\n    return sentence_emotion_map","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T09:06:38.839113Z","iopub.execute_input":"2025-04-08T09:06:38.839448Z","iopub.status.idle":"2025-04-08T09:06:38.845323Z","shell.execute_reply.started":"2025-04-08T09:06:38.839423Z","shell.execute_reply":"2025-04-08T09:06:38.844035Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Function to select background music based on dominant emotion\ndef select_background_music(dominant_emotion):\n    dataset_path = \"/kaggle/input/background-music\"\n    music_tracks = {\n        \"joy\": \"joyful_music.mp3\", \"anger\": \"intense_music.mp3\", \"sadness\": \"sad_music.mp3\", \n        \"fear\": \"suspense_music.mp3\", \"surprise\": \"mystery_music.mp3\", \"excitement\": \"energetic_music.mp3\",\n        \"neutral\": \"calm_music.mp3\"\n    }\n    return os.path.join(dataset_path, music_tracks.get(dominant_emotion, \"calm_music.mp3\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T09:06:40.819062Z","iopub.execute_input":"2025-04-08T09:06:40.819409Z","iopub.status.idle":"2025-04-08T09:06:40.824431Z","shell.execute_reply.started":"2025-04-08T09:06:40.819382Z","shell.execute_reply":"2025-04-08T09:06:40.823375Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Function to remove long silences while keeping natural pauses\ndef remove_silence(input_file, output_file, silence_thresh=-40, min_silence_len=700, max_silence_len=2000):\n    audio = AudioSegment.from_mp3(input_file)\n    silent_ranges = silence.detect_silence(audio, min_silence_len=min_silence_len, silence_thresh=silence_thresh)\n    \n    processed_audio = AudioSegment.empty()\n    prev_end = 0\n    \n    for start, end in silent_ranges:\n        processed_audio += audio[prev_end:start]\n        pause_length = min(end - start, max_silence_len)\n        processed_audio += AudioSegment.silent(duration=pause_length)\n        prev_end = end\n    \n    processed_audio += audio[prev_end:]\n    processed_audio.export(output_file, format=\"wav\")\n    print(f\"Processed audio saved as {output_file}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T09:06:43.024376Z","iopub.execute_input":"2025-04-08T09:06:43.024701Z","iopub.status.idle":"2025-04-08T09:06:43.031016Z","shell.execute_reply.started":"2025-04-08T09:06:43.024675Z","shell.execute_reply":"2025-04-08T09:06:43.029930Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Function to add background music with fade in/out\ndef add_background_music(narration_file, output_file, emotion_map):\n    narration = AudioSegment.from_mp3(narration_file).set_channels(1).set_frame_rate(16000)\n    emotion_list = [emotion for _, emotion in emotion_map.values()]\n    dominant_emotion = max(set(emotion_list), key=emotion_list.count)\n\n    background_music_file = select_background_music(dominant_emotion)\n    background_music = AudioSegment.from_mp3(background_music_file).set_channels(1).set_frame_rate(16000)\n    \n    background_music = background_music - 15  # Louder background music\n\n    background_music = background_music[:len(narration)]  # Match length\n    background_music = background_music.fade_in(3000).fade_out(3000)  # Apply fades\n    final_audio = narration.overlay(background_music)\n    \n    final_audio.export(output_file, format=\"wav\")\n    print(f\"Final audio saved as {output_file}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T09:06:45.240076Z","iopub.execute_input":"2025-04-08T09:06:45.240425Z","iopub.status.idle":"2025-04-08T09:06:45.246680Z","shell.execute_reply.started":"2025-04-08T09:06:45.240396Z","shell.execute_reply":"2025-04-08T09:06:45.245706Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":" #Full execution pipeline\ninput_text = \"Welcome, everyone. Today, we begin with Yogaasana — the gentle art of uniting breath, body, and mind. As you settle onto your mat, let go of any rush or worry. Each posture is not just movement, but a moment of stillness within. Breathe deeply, and allow yourself to simply be. Let the calm flow through you... and trust your body to guide you gently.\"  # Placeholder for ASR\nraw_audio = generate_speech(input_text)\ncleaned_audio = \"cleaned_narration.wav\"\nremove_silence(raw_audio, cleaned_audio)\nemotion_map = detect_emotions(input_text)\nadd_background_music(cleaned_audio, \"final_output.wav\", emotion_map)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T09:06:47.164582Z","iopub.execute_input":"2025-04-08T09:06:47.164985Z","iopub.status.idle":"2025-04-08T09:07:02.695382Z","shell.execute_reply.started":"2025-04-08T09:06:47.164944Z","shell.execute_reply":"2025-04-08T09:07:02.694380Z"}},"outputs":[{"name":"stdout","text":"Generated speech saved as generated_narration.mp3\nProcessed audio saved as cleaned_narration.wav\nFinal audio saved as final_output.wav\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}